---
phase: 01.1-web-scraping-revamp
plan: 02
subsystem: scraper
tags: [prisma, deduplication, google-places-api, hours-extraction]

# Dependency graph
requires:
  - phase: 01.1-01
    provides: Enhanced scraper infrastructure with signal detection
provides:
  - Cross-table deduplication checking both ScrapedBusiness and Business tables
  - Hours extraction from Google Places API opening_hours field
  - Database-level duplicate prevention in scraper save flow
affects: [01.1-03-review-validation, 01.1-04-business-transfer, business-claiming]

# Tech tracking
tech-stack:
  added: []
  patterns:
    - Cross-table duplicate detection pattern
    - Metadata storage for scraped hours data

key-files:
  created: []
  modified:
    - lib/scraper/utils.ts
    - lib/scraper/scraper.ts
    - app/api/admin/scraper/run/route.ts

key-decisions:
  - "Store hours in metadata field as Record<string,string> since Google provides string format"
  - "Check ScrapedBusiness table first, then Business table for deduplication"
  - "Use contains match for name+city in Business table (more flexible than exact)"

patterns-established:
  - "Database deduplication: always check both ScrapedBusiness and Business tables before creating"
  - "Metadata storage: use metadata Json field for scraped data that doesn't fit structured types"

# Metrics
duration: 4min
completed: 2026-01-21
---

# Phase 1.1 Plan 2: Deduplication & Hours Extraction Summary

**Cross-table deduplication preventing scraped duplicates from matching existing Business table records, plus Google Places hours extraction stored in metadata**

## Performance

- **Duration:** 4 min
- **Started:** 2026-01-21T19:55:02Z
- **Completed:** 2026-01-21T19:58:51Z
- **Tasks:** 3
- **Files modified:** 3

## Accomplishments
- Added checkForDuplicateInDatabase function that queries both ScrapedBusiness AND Business tables
- Enhanced transformGooglePlace to extract opening_hours.weekday_text from Google Places API
- Wired cross-table deduplication into scraper save flow with detailed logging
- Hours data now stored in metadata field for scraped businesses

## Task Commits

Each task was committed atomically:

1. **Task 1: Add cross-table deduplication** - `a859066` (feat)
2. **Task 2: Extract hours from Google Places API** - `e74753a` (feat)
3. **Task 3: Wire deduplication into scraper save flow** - `820e5a5` (feat)

## Files Created/Modified
- `lib/scraper/utils.ts` - Added checkForDuplicateInDatabase async function
- `lib/scraper/scraper.ts` - Added hours extraction in transformGooglePlace, sample hours in mock data
- `app/api/admin/scraper/run/route.ts` - Replaced simple dedup with cross-table check, added hours to metadata

## Decisions Made
- **Hours storage in metadata:** Google Places returns hours as human-readable strings ("9:00 AM - 9:00 PM") which don't match the structured BusinessHours type. Storing in metadata.hours preserves the data without forcing type conversion.
- **Contains vs equals for Business table name match:** Using `contains` with case-insensitive mode for name matching in Business table allows catching businesses with slightly different names (e.g., "Al-Noor" vs "Al-Noor Grill")
- **Check ScrapedBusiness first:** Since scraped entries are more likely to be duplicates of other scraped entries, checking that table first is a performance optimization

## Deviations from Plan

### Auto-fixed Issues

**1. [Rule 1 - Bug] Hours type mismatch**
- **Found during:** Task 2 (Extract hours from Google Places API)
- **Issue:** Plan specified storing hours as `Record<string, string>` directly on ScrapedBusiness, but the type interface has `hours?: BusinessHours` with structured DayHours objects
- **Fix:** Stored hours in metadata field instead, which accepts `Record<string, any>`
- **Files modified:** lib/scraper/scraper.ts
- **Verification:** TypeScript compiles without errors
- **Committed in:** e74753a (Task 2 commit)

---

**Total deviations:** 1 auto-fixed (1 bug - type mismatch)
**Impact on plan:** Minor deviation to store hours in metadata instead of typed field. Data is preserved correctly.

## Issues Encountered
None - all tasks completed as expected after the type adjustment.

## User Setup Required
None - no external service configuration required.

## Next Phase Readiness
- Cross-table deduplication ready for production scraping
- Hours extraction working for Google Places API
- Ready for Plan 3: Review and validation workflow

---
*Phase: 01.1-web-scraping-revamp*
*Completed: 2026-01-21*
