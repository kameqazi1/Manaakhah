# Phase 1.1: Web Scraping Revamp - Research

**Researched:** 2026-01-21
**Domain:** Web scraping, business validation, data pipeline
**Confidence:** HIGH (based on codebase analysis)

## Summary

The existing web scraping system is **well-architected but incomplete**. It has a comprehensive scraper infrastructure (`lib/scraper/`) with multi-source support, confidence scoring, deduplication, and signal analysis. However, the pipeline from scraping to the public `/businesses` page has critical gaps:

1. **Scraped businesses are stored separately** in `ScrapedBusiness` table, not the main `Business` table
2. **Approval creates businesses but is missing key fields** - images, hours, services are not transferred
3. **No business validation** exists - scraped data is assumed to be a real business
4. **Public businesses API** only queries `Business` table with `status: PUBLISHED`

The existing infrastructure is solid. The revamp needs to: (a) add business validation, (b) fix the approval-to-business data transfer, (c) ensure images/URLs are properly captured and transferred, (d) verify deduplication works against existing Business entries.

**Primary recommendation:** Enhance the existing pipeline rather than rebuild - the scraper core is good, the gaps are in validation and the approval->Business transfer.

## Current State Analysis

### What Exists

| Component | Location | Status |
|-----------|----------|--------|
| Multi-source scraper | `lib/scraper/scraper.ts` | Complete - 7+ sources |
| Type definitions | `lib/scraper/types.ts` | Complete - comprehensive types |
| Signal analysis | `lib/scraper/utils.ts` | Complete - Muslim keyword scoring |
| Deduplication | `lib/scraper/utils.ts:596-660` | Complete - name/address/phone |
| Admin scraper UI | `app/admin/businesses/scraper/page.tsx` | Complete - 1137 lines |
| Scraped storage | `ScrapedBusiness` Prisma model | Complete |
| Review queue UI | `app/admin/businesses/review-queue/page.tsx` | Complete |
| Approval API | `app/api/admin/scraped-businesses/[id]/route.ts` | Partial - missing fields |
| Public API | `app/api/businesses/route.ts` | Complete - queries Business table |

### Data Flow (Current)

```
1. Admin runs scraper → /api/admin/scraper/run
   ↓
2. scrapeMuslimBusinesses() orchestrates sources
   ↓
3. Results saved to ScrapedBusiness table (PENDING_REVIEW)
   ↓
4. Admin reviews at /admin/businesses/review-queue
   ↓
5. Approval triggers PUT /api/admin/scraped-businesses/[id]
   ↓
6. Creates Business entry from ScrapedBusiness
   ↓
7. Business appears on /api/businesses (PUBLISHED status)
```

### Gap Analysis

| Must-Have | Current State | Gap |
|-----------|--------------|-----|
| Validate scraped data is a business | No validation | Need: Business type classifier |
| Businesses display on /businesses | Works after approval | Pipeline works |
| Name extracted correctly | Works via API parsing | Good |
| Images scraped when available | Captured in `photos` array | NOT transferred on approval |
| Website URL captured | Captured in `website` field | Works |
| Address/phone/hours populated | Address/phone work; hours NOT captured | Need: Extract hours from source |
| Deduplication | Works within scrape batch | NOT checked against Business table |

## Standard Stack

The existing stack is appropriate for this domain:

### Core (Already in Use)
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| Prisma | 6.2.1 | Database ORM | Already in codebase, handles relations |
| Zod | 3.24.1 | Validation | Already in codebase for schema validation |
| Next.js API Routes | 16 | API layer | Already in codebase |

### Supporting (Consider Adding)
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| string-similarity | 4.0.4 | Fuzzy matching | Enhanced deduplication |
| validator | 13.12.0 | Data validation | URL/phone validation |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| Custom Levenshtein | string-similarity | Library is more tested, but current impl works |
| Manual URL validation | validator lib | Library handles edge cases better |

**No new major dependencies needed** - the existing scraper infrastructure is comprehensive.

## Architecture Patterns

### Recommended Enhancement Structure

```
lib/scraper/
├── scraper.ts           # (existing) Multi-source orchestration
├── types.ts             # (existing) Type definitions
├── utils.ts             # (existing) Signal analysis, dedup
├── validation.ts        # (NEW) Business validation logic
└── transfer.ts          # (NEW) ScrapedBusiness -> Business transfer

app/api/admin/
├── scraper/run/route.ts           # (existing) Trigger scraping
├── scraped-businesses/
│   ├── route.ts                   # (existing) List scraped
│   ├── [id]/route.ts              # (ENHANCE) Approval with full transfer
│   └── validate/route.ts          # (NEW) Trigger validation
```

### Pattern 1: Business Validation Pipeline

**What:** Multi-signal validation that a scraped entry is a real business
**When to use:** Before or during admin review

```typescript
// lib/scraper/validation.ts
interface ValidationResult {
  isLikelyBusiness: boolean;
  confidence: number;
  signals: ValidationSignal[];
  flags: string[];
}

interface ValidationSignal {
  type: 'positive' | 'negative';
  signal: string;
  weight: number;
}

export function validateBusinessEntry(scraped: ScrapedBusiness): ValidationResult {
  const signals: ValidationSignal[] = [];

  // Positive signals
  if (scraped.phone && isValidPhoneFormat(scraped.phone)) {
    signals.push({ type: 'positive', signal: 'valid_phone', weight: 20 });
  }
  if (scraped.website && isValidUrl(scraped.website)) {
    signals.push({ type: 'positive', signal: 'has_website', weight: 15 });
  }
  if (scraped.address && hasStreetAddress(scraped.address)) {
    signals.push({ type: 'positive', signal: 'valid_address', weight: 20 });
  }
  if (hasBusinessNamePattern(scraped.name)) {
    signals.push({ type: 'positive', signal: 'business_name_pattern', weight: 10 });
  }

  // Negative signals (spam/residential indicators)
  if (containsResidentialIndicators(scraped.address)) {
    signals.push({ type: 'negative', signal: 'residential_address', weight: -30 });
  }
  if (containsSpamIndicators(scraped.name, scraped.description)) {
    signals.push({ type: 'negative', signal: 'spam_indicators', weight: -40 });
  }
  if (isGenericName(scraped.name)) {
    signals.push({ type: 'negative', signal: 'generic_name', weight: -15 });
  }

  const score = signals.reduce((sum, s) => sum + s.weight, 50); // Base 50

  return {
    isLikelyBusiness: score >= 60,
    confidence: Math.max(0, Math.min(100, score)),
    signals,
    flags: signals.filter(s => s.type === 'negative').map(s => s.signal),
  };
}
```

### Pattern 2: Enhanced Approval Transfer

**What:** Complete data transfer from ScrapedBusiness to Business
**When to use:** When admin approves a scraped business

```typescript
// In app/api/admin/scraped-businesses/[id]/route.ts
if (claimStatus === "APPROVED") {
  const scraped = await db.scrapedBusiness.findUnique({ where: { id: businessId } });

  // Full data transfer including metadata
  const metadata = scraped.metadata as ScrapedMetadata;

  const newBusiness = await db.business.create({
    data: {
      name: scraped.name,
      slug: generateSlug(scraped.name),
      description: scraped.description || `${scraped.name} in ${scraped.city}`,
      category: scraped.category,
      address: scraped.address,
      city: scraped.city,
      state: scraped.state,
      zipCode: scraped.zipCode,
      latitude: scraped.latitude || 37.5485,
      longitude: scraped.longitude || -121.9886,
      phone: scraped.phone || "",
      email: scraped.email,
      website: scraped.website,

      // NEW: Transfer additional data
      hours: metadata?.hours,
      priceRange: metadata?.priceRange,
      serviceList: metadata?.serviceList || [],

      ownerId: "system",
      status: "PUBLISHED",
      verificationStatus: "UNVERIFIED",
      isScraped: true,
      scrapedBusinessId: scraped.id,
      scrapedFrom: metadata?.source,
      scrapedAt: scraped.scrapedAt,
      confidenceScore: metadata?.confidence,
    },
  });

  // Transfer photos
  if (metadata?.photos?.length) {
    await db.businessPhoto.createMany({
      data: metadata.photos.map((photo, index) => ({
        businessId: newBusiness.id,
        url: photo.url,
        type: photo.type || 'general',
        order: index,
        isApproved: true,
      })),
    });
  }

  // Transfer tags
  if (metadata?.tags?.length) {
    await db.businessTagRelation.createMany({
      data: metadata.tags.map(tag => ({
        businessId: newBusiness.id,
        tag: tag as BusinessTag,
      })),
    });
  }
}
```

### Anti-Patterns to Avoid

- **Auto-publishing without review:** Never automatically move scraped data to public without human review
- **Ignoring validation scores:** Don't present all scraped businesses equally - surface flags
- **Incomplete transfer:** Don't lose metadata (photos, tags, hours) during approval
- **Duplicate creation:** Always check against existing Business entries, not just ScrapedBusiness

## Don't Hand-Roll

Problems that look simple but have existing solutions:

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| URL validation | Regex-based URL check | `validator` library or URL constructor | Edge cases (ports, unicode, protocols) |
| Phone normalization | Custom parsing | Existing `normalizePhone()` in utils.ts | Already handles US formats |
| Fuzzy string matching | Basic includes() | Existing `calculateSimilarity()` | Levenshtein already implemented |
| Slug generation | Simple replace | Consider `slugify` lib | Already have basic impl that works |
| Geocoding | Manual API calls | Existing `geocodeAddress()` | Already handles Mapbox fallback |

**Key insight:** The scraper utilities (`lib/scraper/utils.ts`) already have most helper functions needed. Use them.

## Common Pitfalls

### Pitfall 1: Scraper Returns Mock Data in Production

**What goes wrong:** When API keys are missing, scraper returns mock/fake data
**Why it happens:** `scrapeGooglePlaces()` falls back to `generateMockBusinesses()` if no API key
**How to avoid:**
- Add explicit production check: throw error if API key missing in production
- Log warnings clearly when mock data is generated
- Mark mock-generated businesses in metadata
**Warning signs:** High volume of identical-looking businesses, fake phone numbers

### Pitfall 2: Duplicate Businesses Created

**What goes wrong:** Same business exists in both ScrapedBusiness and Business tables
**Why it happens:** Current dedup only checks within ScrapedBusiness, not against Business
**How to avoid:**
```typescript
// Before saving scraped business, also check Business table
const existingBusiness = await db.business.findFirst({
  where: {
    OR: [
      { name: business.name, address: business.address },
      { phone: business.phone },
    ],
  },
});
if (existingBusiness) {
  // Skip or flag as potential duplicate
}
```
**Warning signs:** Same business name appearing multiple times with slight variations

### Pitfall 3: Photos Lost on Approval

**What goes wrong:** Scraped photos exist in metadata but Business has no images
**Why it happens:** Current approval code doesn't transfer `metadata.photos` array
**How to avoid:** Explicitly create `BusinessPhoto` records during approval
**Warning signs:** Approved businesses showing placeholder images

### Pitfall 4: Residential Addresses as Businesses

**What goes wrong:** Home addresses scraped as business locations
**Why it happens:** Google Places returns home-based businesses without filtering
**How to avoid:**
- Check for residential indicators: "Apt", "Suite" without business indicators
- Cross-reference with business registry data if available
- Flag addresses that are purely residential zones
**Warning signs:** Address patterns like "123 Main St Apt 4"

### Pitfall 5: Category Mismatch After Transfer

**What goes wrong:** ScrapedBusiness uses scraper types, Business uses Prisma enum
**Why it happens:** Type definitions in `lib/scraper/types.ts` may drift from Prisma schema
**How to avoid:** Import BusinessCategory from Prisma, not from scraper types
**Warning signs:** TypeScript errors on category assignment, "OTHER" fallback used too often

## Code Examples

### Example 1: Business Validation Check

```typescript
// Source: New implementation based on existing patterns
function containsResidentialIndicators(address: string): boolean {
  const residential = [
    /\bapt\.?\s*#?\s*\d+/i,
    /\bunit\s*#?\s*\d+/i,
    /\b(suite|ste)\.?\s*(?!\d{3})\d{1,2}\b/i, // Suite 1-99, not Suite 100+
    /\bfloor\s*\d+/i,
    /\bbasement\b/i,
    /\bgarage\b/i,
  ];
  return residential.some(pattern => pattern.test(address));
}

function hasBusinessNamePattern(name: string): boolean {
  const businessIndicators = [
    /\b(inc|llc|corp|ltd|co|company)\b/i,
    /\b(restaurant|cafe|grill|kitchen|market|grocery|store|shop|salon|center)\b/i,
    /\b(services|solutions|consulting|agency)\b/i,
    /\bmasjid\b/i,
    /\bmosque\b/i,
  ];
  return businessIndicators.some(pattern => pattern.test(name));
}

function containsSpamIndicators(name: string, description?: string): boolean {
  const combined = `${name} ${description || ''}`.toLowerCase();
  const spam = [
    /\b(work from home|earn money|get rich)\b/i,
    /\b(click here|visit now|limited time)\b/i,
    /\b(free|discount|deal)\s+(code|offer|special)\b/i,
    /\$\d+[kK]?\s*(per|a)\s*(month|week|day)/i,
  ];
  return spam.some(pattern => pattern.test(combined));
}
```

### Example 2: Cross-Table Deduplication

```typescript
// Source: Enhancement to existing checkForDuplicate
export async function checkForDuplicateInDatabase(
  business: ScrapedBusiness
): Promise<{ isDuplicate: boolean; existingId?: string; matchType?: string }> {
  // Check ScrapedBusiness table (existing)
  const existingScraped = await db.scrapedBusiness.findFirst({
    where: {
      OR: [
        { name: business.name, address: business.address },
        business.phone ? { phone: business.phone } : {},
      ].filter(obj => Object.keys(obj).length > 0),
    },
  });

  if (existingScraped) {
    return { isDuplicate: true, existingId: existingScraped.id, matchType: 'scraped' };
  }

  // NEW: Also check Business table
  const existingBusiness = await db.business.findFirst({
    where: {
      OR: [
        {
          name: { contains: business.name, mode: 'insensitive' },
          city: business.city,
        },
        business.phone ? { phone: business.phone } : {},
      ].filter(obj => Object.keys(obj).length > 0),
    },
  });

  if (existingBusiness) {
    return { isDuplicate: true, existingId: existingBusiness.id, matchType: 'business' };
  }

  return { isDuplicate: false };
}
```

### Example 3: Complete Photo Transfer

```typescript
// Source: Enhancement to approval handler
async function transferPhotosToBusinessSafely(
  businessId: string,
  scrapedPhotos: ScrapedPhoto[] | undefined
): Promise<number> {
  if (!scrapedPhotos || scrapedPhotos.length === 0) return 0;

  const validPhotos = scrapedPhotos.filter(photo => {
    // Validate URL format
    try {
      new URL(photo.url);
      return true;
    } catch {
      return false;
    }
  });

  if (validPhotos.length === 0) return 0;

  await db.businessPhoto.createMany({
    data: validPhotos.map((photo, index) => ({
      businessId,
      url: photo.url,
      caption: photo.caption || null,
      type: photo.type || 'general',
      order: index,
      isApproved: true, // Pre-approved from trusted source
    })),
  });

  return validPhotos.length;
}
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Single-source scraping | Multi-source orchestration | Already implemented | Better coverage |
| Manual categorization | Keyword-based auto-categorization | Already implemented | Faster review |
| No confidence scoring | Signal-based scoring | Already implemented | Quality filtering |

**Deprecated/outdated:**
- None - the existing scraper is relatively modern

**Current best practices already followed:**
- Async/await throughout
- TypeScript with comprehensive types
- Confidence scoring for quality
- Deduplication with fuzzy matching
- Rate limiting built-in

## Implementation Priorities

Based on the must-haves:

### Priority 1: Fix Approval Data Transfer (Highest Impact)
- Currently: Creates Business but loses metadata (photos, tags, hours)
- Fix: Enhance PUT handler to transfer all fields
- Files: `app/api/admin/scraped-businesses/[id]/route.ts`

### Priority 2: Add Business Validation
- Currently: No validation that scraped data is a real business
- Fix: Add validation module, surface flags in review queue
- Files: New `lib/scraper/validation.ts`, enhance review queue UI

### Priority 3: Cross-Table Deduplication
- Currently: Only deduplicates within ScrapedBusiness table
- Fix: Also check against Business table before saving
- Files: `app/api/admin/scraper/run/route.ts`

### Priority 4: Hours Extraction (Nice to Have)
- Currently: Hours not extracted from Google Places
- Fix: Parse `opening_hours` from API response
- Files: `lib/scraper/scraper.ts` in `transformGooglePlace()`

## Open Questions

Things that couldn't be fully resolved:

1. **API Key Availability**
   - What we know: Scraper falls back to mock data without keys
   - What's unclear: Which API keys are configured in production?
   - Recommendation: Verify GOOGLE_PLACES_API_KEY and YELP_API_KEY exist

2. **Image Storage**
   - What we know: Photos are URLs from source (e.g., Google CDN)
   - What's unclear: Should we copy to our own storage? CDN URLs may expire
   - Recommendation: For now, keep source URLs but consider future migration to Cloudinary

3. **Verification Workflow**
   - What we know: Scraped businesses get `verificationStatus: UNVERIFIED`
   - What's unclear: Is there a downstream verification process?
   - Recommendation: Unverified status is appropriate for scraped data

## Sources

### Primary (HIGH confidence)
- Codebase analysis: `lib/scraper/*`, `app/api/admin/scraped-businesses/*`
- Prisma schema: `prisma/schema.prisma` - ScrapedBusiness and Business models
- Existing implementation: Full review of current data flow

### Secondary (MEDIUM confidence)
- Google Places API documentation patterns (from existing code implementation)

### Tertiary (LOW confidence)
- None - this research is entirely based on codebase analysis

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH - existing stack is appropriate
- Architecture: HIGH - based on direct code analysis
- Pitfalls: HIGH - identified from actual code gaps
- Implementation approach: HIGH - builds on existing patterns

**Research date:** 2026-01-21
**Valid until:** 90 days (stable domain, codebase changes may occur)
